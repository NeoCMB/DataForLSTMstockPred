{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow, as the name indicates, is a framework to define and run computations involving tensors. A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation command\n",
    "#!pip install tensorflow\n",
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=3 #[treated as a 0-D tensor]\n",
    "a=[3,5] #[treated as a 1-D tensor]\n",
    "a=[[3,5],[1,6]] # treated as a 2-D sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tf.Tensor has the following properties:\n",
    "    1. a data type (float32, int32, or string, for example)\n",
    "    2. a shape\n",
    "Some types of tensors are special, the main ones are:\n",
    "    1. tf.Variable\n",
    "    2. tf.constant\n",
    "    3. tf.placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(3.0, dtype=tf.float32)\n",
    "b = tf.constant(4.0) # also tf.float32 implicitly\n",
    "total = a + b\n",
    "print(a)\n",
    "print(b)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that printing the tensors does not output the values 3.0, 4.0, and 7.0 as you might expect. The above statements only build the computation graph. These tf.Tensor objects just represent the results of the operations that will be run.\n",
    "\n",
    "   To evaluate tensors, instantiate a tf.Session object, informally known as a session. A session encapsulates the state of the TensorFlow runtime, and runs TensorFlow operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(3.0, dtype=tf.float32)\n",
    "b = tf.constant(4.0) # also tf.float32 implicitly\n",
    "sum_1 = a + b\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(b))\n",
    "    print(sess.run(sum_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(3.0, dtype=tf.float32)\n",
    "b = tf.constant(4.0) \n",
    "total = a + b\n",
    "writer = tf.summary.FileWriter('vp1')\n",
    "writer.add_graph(tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable......\n",
    "a=tf.Variable(6)\n",
    "b=tf.Variable(5)\n",
    "f=a+b\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(sess.run(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant....\n",
    "import tensorflow as tf\n",
    "a=tf.constant([1,2,3])\n",
    "b=tf.constant([5,6,7])\n",
    "z=tf.multiply(a,b)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## place holders............\n",
    "import tensorflow as tf\n",
    "a=tf.placeholder(tf.float32)\n",
    "b=tf.placeholder(tf.float32)\n",
    "z=a+b\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(z,feed_dict={a:[1,2,3],b:[1,2,3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "w=tf.Variable([.3],tf.float32)\n",
    "b=tf.Variable([-.3],tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output \n",
    "x=tf.placeholder(tf.float32)\n",
    "y=tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "linear_model=w*x+b\n",
    "\n",
    "# loss\n",
    "squared_delta=tf.square(linear_model-y)\n",
    "loss=tf.reduce_sum(squared_delta)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(loss,{x:[1,2,3,4],y:[0,-1,-2,-3]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear regression with optimization\n",
    "\n",
    "w=tf.Variable([.3],tf.float32)\n",
    "b=tf.Variable([-.3],tf.float32)\n",
    "x=tf.placeholder(tf.float32)\n",
    "linear_model=w*x+b\n",
    "y=tf.placeholder(tf.float32)\n",
    "squared_delta=tf.square(linear_model-y)\n",
    "loss=tf.reduce_sum(squared_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimizer\n",
    "optimizer=tf.train.GradientDescentOptimizer(0.01)\n",
    "train=optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(0,1000):\n",
    "        sess.run(train,{x:[1,2,3,4],y:[0,-1,-2,-3]})\n",
    "        if i%100==0:\n",
    "            print('loss:', sess.run(loss,{x:[1,2,3,4],y:[0,-1,-2,-3]}))\n",
    "    print('final value of W: ', sess.run(w))\n",
    "    print('final value of b: ', sess.run(b))\n",
    "    print('final loss:', sess.run(loss,{x:[1,2,3,4],y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN with MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"/tmp/data\",one_hot=True) # loading data from TenserFlow's data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_nodes_hl1=500\n",
    "n_nodes_hl2=500\n",
    "n_nodes_hl3=500\n",
    "n_classes=10\n",
    "batch=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model's input/ output\n",
    "x=tf.placeholder('float',[None,784])\n",
    "y=tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "\n",
    "def neuralnet(data):\n",
    "    hidden_layer1={'weights':tf.Variable(tf.random_normal([784,n_nodes_hl1])),\n",
    "                   'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    hidden_layer2={'weights':tf.Variable(tf.random_normal([n_nodes_hl1,n_nodes_hl2])),\n",
    "                  'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    hidden_layer3={'weights':tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_hl3])),\n",
    "                  'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    output_layer={'weights':tf.Variable(tf.random_normal([n_nodes_hl3,n_classes])),\n",
    "                 'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    l1=tf.add(tf.matmul(data,hidden_layer1['weights']),hidden_layer1['biases'])\n",
    "    l1=tf.nn.relu(l1)\n",
    "    l2=tf.add(tf.matmul(l1,hidden_layer2['weights']),hidden_layer2['biases'])\n",
    "    l2=tf.nn.relu(l2)\n",
    "    l3=tf.add(tf.matmul(l2,hidden_layer3['weights']),hidden_layer3['biases'])\n",
    "    l3=tf.nn.relu(l3)\n",
    "    out=tf.add(tf.matmul(l3,output_layer['weights']),output_layer['biases'])\n",
    "    return(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "\n",
    "def train_NN(x):\n",
    "    predict=neuralnet(x)\n",
    "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict,labels=y))\n",
    "    optimizer=tf.train.AdamOptimizer().minimize(cost)\n",
    "    num_epochs=10\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss=0\n",
    "            for i in range(int(mnist.train.num_examples/batch)):\n",
    "                batch_x,batch_y=mnist.train.next_batch(batch)\n",
    "                _,batch_cost=sess.run([optimizer,cost], feed_dict={x:batch_x,y:batch_y})\n",
    "                epoch_loss+=batch_cost\n",
    "            print('epoch', epoch, 'completed out of', num_epochs, 'loss', epoch_loss )\n",
    "        correct=tf.equal(tf.arg_max(predict,1),tf.arg_max(y,1))\n",
    "        accuracy=tf.reduce_mean(tf.cast(correct,'float'))\n",
    "        print('Accuracy:',accuracy.eval({x:mnist.test.images,y:mnist.test.labels}))\n",
    "\n",
    "        \n",
    "train_NN(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"/tmp/data\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "x=tf.placeholder(float,[None,784])\n",
    "y=tf.placeholder(float)\n",
    "n_classes=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2D(x,w):\n",
    "    return(tf.nn.conv2d(x,w,strides=[1,1,1,1],padding='SAME'))\n",
    "def maxpool2D(x):\n",
    "    return(tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_net(x):\n",
    "    weights={'W_conv1':tf.Variable(tf.random_normal([5,5,1,32])),\n",
    "             'W_conv2':tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "             'W_fc':tf.Variable(tf.random_normal([7*7*64,1024])),\n",
    "             'W_out':tf.Variable(tf.random_normal([1024,n_classes]))\n",
    "              }\n",
    "    \n",
    "    biases={'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "            'b_conv2':tf.Variable(tf.random_normal([64])),\n",
    "            'b_fc':tf.Variable(tf.random_normal([1024])),\n",
    "            'b_out':tf.Variable(tf.random_normal([n_classes]))\n",
    "        }\n",
    "    x=tf.reshape(x,shape=[-1,28,28,1])\n",
    "    layer1=conv2D(x,weights['W_conv1'])+biases['b_conv1']\n",
    "    layer1=maxpool2D(layer1)\n",
    "    layer1=tf.nn.relu(layer1)\n",
    "    \n",
    "    layer2=conv2D(layer1,weights['W_conv2'])+biases['b_conv2']\n",
    "    layer2=maxpool2D(layer2)\n",
    "    layer2=tf.nn.relu(layer2)\n",
    "    \n",
    "    \n",
    "    layer_fc=tf.reshape(layer2,[-1,7*7*64])\n",
    "    layer_fc=tf.add(tf.matmul(layer_fc,weights['W_fc']),biases['b_fc'])\n",
    "    layer_fc=tf.nn.relu(layer_fc)\n",
    "    \n",
    "    layer_out=tf.add(tf.matmul(layer_fc,weights['W_out']),biases['b_out'])\n",
    "    return(layer_out)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn_net(x):\n",
    "    predict=cnn_net(x)\n",
    "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict,labels=y))\n",
    "    optimizer=tf.train.AdamOptimizer().minimize(cost)\n",
    "    num_epochs=10\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss=0\n",
    "            for i in range(int(mnist.train.num_examples/batch_size)):\n",
    "                apoch_x,apoch_y=mnist.train.next_batch(batch_size)\n",
    "                k,c=sess.run([optimizer,cost], feed_dict={x:apoch_x,y:apoch_y})\n",
    "                epoch_loss+=c\n",
    "            print('epoch', epoch, 'completed out of', num_epochs, 'loss', epoch_loss )\n",
    "        correct=tf.equal(tf.arg_max(predict,1),tf.arg_max(y,1))\n",
    "        accuracy=tf.reduce_mean(tf.cast(correct,'float'))\n",
    "        print('Accuracy:',accuracy.eval({x:mnist.test.images,y:mnist.test.labels}))\n",
    "\n",
    "        \n",
    "train_cnn_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "mnist=input_data.read_data_sets(\"/tmp/data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs=784    #28x28 pixels\n",
    "num_hid1=392\n",
    "num_hid2=196\n",
    "num_hid3=num_hid1\n",
    "num_output=num_inputs\n",
    "lr=0.01\n",
    "actf=tf.nn.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32,shape=[None,num_inputs])\n",
    "initializer=tf.variance_scaling_initializer()\n",
    "\n",
    "w1=tf.Variable(initializer([num_inputs,num_hid1]),dtype=tf.float32)\n",
    "w2=tf.Variable(initializer([num_hid1,num_hid2]),dtype=tf.float32)\n",
    "w3=tf.Variable(initializer([num_hid2,num_hid3]),dtype=tf.float32)\n",
    "w4=tf.Variable(initializer([num_hid3,num_output]),dtype=tf.float32)\n",
    "\n",
    "b1=tf.Variable(tf.zeros(num_hid1))\n",
    "b2=tf.Variable(tf.zeros(num_hid2))\n",
    "b3=tf.Variable(tf.zeros(num_hid3))\n",
    "b4=tf.Variable(tf.zeros(num_output))\n",
    "\n",
    "hid_layer1=actf(tf.matmul(X,w1)+b1)\n",
    "hid_layer2=actf(tf.matmul(hid_layer1,w2)+b2)\n",
    "hid_layer3=actf(tf.matmul(hid_layer2,w3)+b3)\n",
    "output_layer=actf(tf.matmul(hid_layer3,w4)+b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(tf.square(output_layer-X))\n",
    "\n",
    "optimizer=tf.train.AdamOptimizer(lr)\n",
    "train=optimizer.minimize(loss)\n",
    "\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch=5\n",
    "batch_size=150\n",
    "num_test_images=10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        num_batches=mnist.train.num_examples//batch_size\n",
    "        for iteration in range(num_batches):\n",
    "            X_batch,y_batch=mnist.train.next_batch(batch_size)\n",
    "            sess.run(train,feed_dict={X:X_batch})\n",
    "            \n",
    "        train_loss=loss.eval(feed_dict={X:X_batch})\n",
    "    results=output_layer.eval(feed_dict={X:mnist.test.images[:num_test_images]})\n",
    "\n",
    "print(\"epoch {} loss {}\".format(epoch,train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing original images with reconstructions\n",
    "f,a=plt.subplots(2,10,figsize=(20,4))\n",
    "for i in range(num_test_images):\n",
    "    a[0][i].imshow(np.reshape(mnist.test.images[i],(28,28)))\n",
    "    a[1][i].imshow(np.reshape(results[i],(28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
